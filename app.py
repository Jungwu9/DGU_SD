import streamlit as st
import os, glob, io, re, csv, json, random, datetime
import numpy as np
from io import StringIO

import pdfplumber

# LangChain (RAG)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# Gemini (chat ì „ìš©; ì„ë² ë”©ì€ í´ë°± í¬í•¨)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# HF ì„ë² ë”© í´ë°±
try:
    from langchain_huggingface import HuggingFaceEmbeddings as HFEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings as HFEmbeddings

# Matchingìš©
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import torch


# ===== ê¸°ë³¸ ì„¤ì • =====
st.set_page_config(page_title="ê³¼ì œ ê³µê³ ë¬¸ ìš”ì•½Â·ë§¤ì¹­ê¸°", layout="wide")
st.title("ê³¼ì œ ê³µê³ ë¬¸ ìš”ì•½Â·ë§¤ì¹­ê¸°")
st.markdown("---")

# ì‚¬ì´ë“œë°”
st.sidebar.header("API Key ì„¤ì •")
api_key = st.sidebar.text_input("Google AI Studio API í‚¤", type="password")

st.sidebar.header("í”„ë¡œí•„ íŒŒì¼")
profiles_path = st.sidebar.text_input(
    "êµìˆ˜ í”„ë¡œí•„ JSONL ê²½ë¡œ",
    value=r"C:\Users\PL_LAB_5\Desktop\ì‚°ë‹¨\profiles_updated_exp2.jsonl"
)

st.sidebar.caption("â€» PDF ì—…ë¡œë“œ ì¦‰ì‹œ ìš”ì•½â†’ë§¤ì¹­ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

if not api_key:
    st.sidebar.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. (https://aistudio.google.com/app/apikey)")
    st.stop()

# í™˜ê²½ ë³€ìˆ˜
os.environ["GOOGLE_API_KEY"] = api_key
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# ê²½ë¡œ & ìƒìˆ˜
BASE_DATA_PATH = r"C:\Users\PL_LAB_5\PyCharmMiscProject\LLaMA-Factory\pi\ntis_selenium"
SBERT_MODEL_FOR_MATCH = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
HF_EMBED_MODEL_FOR_RAG = SBERT_MODEL_FOR_MATCH
TOP_K_PREVIEW = 30

# ===== ìš”ì•½ í”„ë¡¬í”„íŠ¸ =====
def get_prompt_template():
    return """
## ì§€ì‹œì‚¬í•­ (Instruction)
ë‹¹ì‹ ì€ êµ­ê°€ ì—°êµ¬ê°œë°œ ê³¼ì œ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” **ì „ë¬¸ ì—°êµ¬ ë¶„ì„ê°€**ì…ë‹ˆë‹¤.  
ì£¼ì–´ì§„ ê³µê³ ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì² ì €íˆ ê²€í† í•œ ë’¤, ì•„ë˜ ì œì‹œëœ **ìš”ì•½ ì–‘ì‹**ì— ë”°ë¼ ê° í•­ëª©ì„ **ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ** ì‘ì„±í•´ì£¼ì„¸ìš”.  
ê³µê³ ë¬¸ ê³³ê³³ì— ë¶„ì‚°ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‚´ìš©ì„ êµ¬ì„±í•˜ë©°, ëˆ„ë½ì´ ì—†ë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.  

**ê°€ì¥ ì¤‘ìš”í•˜ê²Œ, ì‘ì„± ì‹œ ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ì¼ê´€ë˜ê³  ì˜¬ë°”ë¥¸ ì–´ë²•ê³¼ ë¬¸ë²•ì„ ìœ ì§€í•´ì£¼ì‹­ì‹œì˜¤.**
- **ëª…í™•ì„± ë° ê°„ê²°ì„±:** ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ë°˜ë³µì ì¸ í‘œí˜„ì„ í”¼í•˜ê³ , í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
- **ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ì–´ì¡°:** ì£¼ê´€ì ì¸ íŒë‹¨ì´ë‚˜ ê°ì •ì ì¸ í‘œí˜„ ì—†ì´, ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ì–´ì¡°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- **ë¬¸ì²´ í†µì¼ì„±:** ëª¨ë“  í•­ëª©ì— ê±¸ì³ í†µì¼ëœ ë¬¸ì²´ì™€ í‘œí˜„ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì½ëŠ” ì´ì—ê²Œ ì¼ê´€ëœ ì¸ìƒì„ ì¤ë‹ˆë‹¤.
- **ì •í™•í•œ ìš©ì–´ ì‚¬ìš©:** ê³µê³ ë¬¸ì— ëª…ì‹œëœ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ë©°, ì˜¤íƒ€ ë° ë¹„ë¬¸ì´ ì—†ë„ë¡ í•©ë‹ˆë‹¤.

---

## ì…ë ¥ ë°ì´í„° (RFP ì›ë¬¸)
{context}

---

## ê³¼ì œ (Task)
êµ­ê°€ ì—°êµ¬ê°œë°œ ê³¼ì œ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì •ë³´ë¥¼ ì•„ë˜ `<ìš”ì•½ ì–‘ì‹>`ì— ë”°ë¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.  
ê° í•­ëª©ì€ ë°˜ë“œì‹œ ê³µê³ ë¬¸ì— ê¸°ë°˜í•˜ì—¬ ì‘ì„±í•´ì•¼ í•˜ë©°, ì¶”ì •ì´ë‚˜ ìœ ì¶”ëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.

(ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸/ì¶”ê°€ ì§€ì‹œì…ë‹ˆë‹¤)
{input}

---

## ì¶œë ¥ ì–‘ì‹ (ìš”ì•½ ê²°ê³¼)

[ì¶”ì¶œ ê²°ê³¼]

### ê³¼ì œ ëª©í‘œ
- ê³¼ì œê°€ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ìµœì¢…ì ì´ê³  í•µì‹¬ì ì¸ ëª©í‘œë¥¼ ëª…í™•í•˜ê²Œ ê¸°ìˆ í•©ë‹ˆë‹¤.
- í•„ìš” ì‹œ, ì •ì„±ì  ëª©í‘œì™€ ì •ëŸ‰ì  ëª©í‘œë¥¼ êµ¬ë¶„í•˜ì—¬ ì„œìˆ í•©ë‹ˆë‹¤.

### ì—°êµ¬ ê¸°ê°„
- ì „ì²´ ì—°êµ¬ ê¸°ê°„ê³¼ ì‹œì‘/ì¢…ë£Œ ì—°ì›”ì„ ëª…í™•íˆ í‘œê¸°í•©ë‹ˆë‹¤. (ì˜ˆ: 2025.07.01 ~ 2029.12.31 (ì´ 54ê°œì›”))

### ê³¼ì œ ì˜ˆì‚°
- ì´ ì—°êµ¬ê°œë°œë¹„, ì •ë¶€ì§€ì›ê¸ˆ, ë¯¼ê°„ë¶€ë‹´ê¸ˆ(ê¸°ê´€ë¶€ë‹´ê¸ˆ) ë“± ì˜ˆì‚° ê´€ë ¨ ì •ë³´ë¥¼ ìƒì„¸íˆ ê¸°ì¬í•©ë‹ˆë‹¤.

### ì§€ì› ìê²© ë° í˜•íƒœ
- **ì§€ì› ìê²©:** ê¸°ì—…(ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—…, ì¤‘ì†Œê¸°ì—…), ëŒ€í•™, ì—°êµ¬ê¸°ê´€, í˜‘íšŒ ë“± ì§€ì› ê°€ëŠ¥í•œ ì£¼ì²´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•©ë‹ˆë‹¤.
- **ì§€ì› í˜•íƒœ:** ì£¼ê´€ê¸°ê´€, ê³µë™ì—°êµ¬ê¸°ê´€, ìœ„íƒì—°êµ¬ê¸°ê´€ ë“± ì°¸ì—¬ í˜•íƒœì™€ ì»¨ì†Œì‹œì—„ êµ¬ì„± ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ëª…í™•íˆ ê¸°ì¬í•©ë‹ˆë‹¤.

### ê³µê³  ìš”ì•½
- ê³¼ì œì˜ ì¶”ì§„ ë°°ê²½, í•„ìš”ì„±, í•µì‹¬ ëª©í‘œ ë° ì£¼ìš” ì—°êµ¬ ë‚´ìš© ë“±ì„ ì¢…í•©í•˜ì—¬ 5ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
- ê¸°ìˆ ì  íŠ¹ì§•ì´ë‚˜ ì •ì±…ì  ì˜ì˜ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ê¸°ìˆ í•©ë‹ˆë‹¤.

### ì‚¬ì—… ë‚´ìš©
- ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° ìš”êµ¬ë˜ëŠ” ê¸°ìˆ ì  í•µì‹¬ ìš”ì†Œì™€ ì£¼ìš” ì¶”ì§„ ë‚´ìš©ì„ ìƒì„¸íˆ ì •ë¦¬í•©ë‹ˆë‹¤.

### ê´€ë ¨ ê¸°ìˆ /ì‚°ì—… ë™í–¥
- ë³¸ ê³¼ì œê°€ ì†í•œ ê¸°ìˆ  ë¶„ì•¼(ì˜ˆ: ì†Œí˜•ëª¨ë“ˆì›ìë¡œ(SMR), ì¸ê³µì§€ëŠ¥(AI), ë°”ì´ì˜¤í—¬ìŠ¤ ë“±)ì˜ ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œ, ì‹œì¥ ë™í–¥, ì •ì±…ì  ì¤‘ìš”ì„± ë“±ì„ ê³µê³ ë¬¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ 1~2ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

### ê¸°ëŒ€ íš¨ê³¼ ë° í™œìš© ë°©ì•ˆ
- **ê¸°ìˆ ì  ê¸°ëŒ€íš¨ê³¼:** ì—°êµ¬ ì„±ê³µ ì‹œ í™•ë³´í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ  ìˆ˜ì¤€ ë° íŒŒê¸‰ íš¨ê³¼
- **ê²½ì œì /ì‚°ì—…ì  ê¸°ëŒ€íš¨ê³¼:** ìˆ˜ì¶œ, ë§¤ì¶œ, ê³ ìš©, ì‹œì¥ ì°½ì¶œ ë“± ê²½ì œÂ·ì‚°ì—…ì  ê¸°ì—¬ ë°©ì•ˆ ë° ê¸°ëŒ€ íš¨ê³¼
- **í™œìš© ë°©ì•ˆ:** ê°œë°œëœ ê¸°ìˆ ì´ ì‹¤ì œ ì–´ë””ì— ì–´ë–»ê²Œ ì ìš©ë  ìˆ˜ ìˆëŠ”ì§€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•©ë‹ˆë‹¤.

### ì£¼ìš” í‰ê°€ í•­ëª©/ì¤‘ì  ì‚¬í•­
- ì„ ì • í‰ê°€ ê¸°ì¤€, ìš°ëŒ€ì‚¬í•­, ê°€ì  í•­ëª© ë“± ê³µê³ ë¬¸ì— ëª…ì‹œëœ í•µì‹¬ í‰ê°€ ìš”ì†Œë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

### í‚¤ì›Œë“œ
- ê³¼ì œì˜ í•µì‹¬ ê¸°ìˆ , ëª©í‘œ, ì ìš© ë¶„ì•¼ ë“±ì„ ë‚˜íƒ€ë‚´ëŠ” í•µì‹¬ ë‹¨ì–´ë¥¼ 10ê°œ ì´í•˜ë¡œ ì‘ì„±í•˜ê³  ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.

â€» ìœ„ í•­ëª©ê¹Œì§€ ì‘ì„±í•œ í›„, ë™ì¼í•œ ë‚´ìš©ì„ ë°˜ë³µí•˜ê±°ë‚˜ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”. ì¶œë ¥ì€ ì—¬ê¸°ì„œ ëì…ë‹ˆë‹¤.
"""

# ===== ìœ í‹¸ =====
def clean_final_output(raw_text: str) -> str:
    start_marker = "### ê³¼ì œ ëª©í‘œ"
    i = raw_text.find(start_marker)
    if i == -1:
        j = raw_text.find("###")
        return raw_text.strip() if j == -1 else raw_text[j:].strip()
    return raw_text[i:].strip()

def find_file_and_get_info(base_path, uploaded_filename):
    search_pattern = os.path.join(base_path, "**", "*")
    for file in glob.glob(search_pattern, recursive=True):
        if os.path.basename(file) == uploaded_filename:
            file_folder = os.path.dirname(file)
            folder_name = os.path.basename(file_folder)
            try:
                with open(os.path.join(file_folder, "department_name.txt"), encoding="utf-8") as f:
                    department = f.read().strip()
            except FileNotFoundError:
                department = "ì •ë³´ ì—†ìŒ"
            try:
                with open(os.path.join(file_folder, "notice_link.txt"), encoding="utf-8") as f:
                    link = f.read().strip()
            except FileNotFoundError:
                link = "ì •ë³´ ì—†ìŒ"
            return True, department, link, folder_name
    return False, "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

def extract_text_from_pdf(pdf_bytes):
    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            text = "\n".join([page.extract_text() or "" for page in pdf.pages])
        if not text.strip():
            raise ValueError("No text found in PDF.")
        return [Document(page_content=text)]
    except Exception as e:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def _format_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

def csv_bytes_from_rows(rows, fieldnames):
    sio = StringIO()
    writer = csv.DictWriter(sio, fieldnames=fieldnames)
    writer.writeheader()
    for r in rows:
        writer.writerow(r)
    return sio.getvalue().encode("utf-8-sig")


# ===== ë§¤ì¹­ íŒŒì´í”„ë¼ì¸ ìƒìˆ˜ / í•¨ìˆ˜ =====

YEAR_WEIGHTS = {2025: 1.5, 2024: 1.2, 2023: 1.1}
DEFAULT_YEAR_WEIGHT = 1.0
STAGE1_WEIGHTS = np.array([0.7, 0.1, 0.1, 0.1], dtype=float)  # (major, researchs, projects, fingerprints)
STAGE2_WEIGHTS = np.array([0.2, 0.5, 0.2, 0.1], dtype=float)  # (major, research_year, projects, fingerprints)

def set_seeds(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def normalize_list(val):
    if isinstance(val, list):
        return val
    if isinstance(val, dict):
        out = []
        for v in val.values():
            out += v if isinstance(v, list) else [str(v)]
        return out
    return [] if val is None else [str(val)]

def load_profiles(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"í”„ë¡œí•„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    profiles = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            p = json.loads(line)
            p['researchs']    = normalize_list(p.get('researchs', []))
            p['projects']     = normalize_list(p.get('projects', []))
            p['fingerprints'] = normalize_list(p.get('fingerprints', []))
            # emailë„ ê°™ì´ ìœ ì§€
            p['email']        = p.get('email', "")
            profiles.append(p)
    return profiles

def find_elbow_threshold(scores: np.ndarray) -> float:
    """2ì°¨ ì°¨ë¶„ ê¸°ë°˜ ì—˜ë³´ìš° ì§€ì  íƒì§€"""
    if scores.size == 0:
        return 0.0
    if scores.size == 1:
        return float(scores[0])
    s = np.sort(scores)[::-1]
    sd2 = np.diff(s, n=2)
    idx = int(np.argmax(np.abs(sd2))) + 1
    return float(s[idx])


def run_matching(summary_text: str, profiles_file: str, top_k_preview: int = TOP_K_PREVIEW):
    """
    summary_text ê¸°ë°˜ìœ¼ë¡œ êµìˆ˜ í”„ë¡œí•„ ë§¤ì¹­.
    í™”ë©´ì—ëŠ” ìƒìœ„ í›„ë³´ë“¤ì˜ scoreì™€ label(True/False)ì„ ê°™ì´ ë³´ì—¬ì£¼ê³ ,
    CSVëŠ” ì—¬ì „íˆ label=True(ì¶”ì²œ)ë§Œ ì €ì¥.
    """
    set_seeds(42)
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    profiles = load_profiles(profiles_file)

    # SBERT ë¡œë“œ & ë¬¸ì„œ(=ìš”ì•½) ì„ë² ë”©
    sbert = SentenceTransformer(SBERT_MODEL_FOR_MATCH, device=DEVICE)
    emb_doc = sbert.encode([summary_text], convert_to_numpy=True, normalize_embeddings=True)

    # ----- Stage1 (major ë¹„ì¤‘ ë†’ìŒ) -----
    majors = [p.get('major','') for p in profiles]
    emb_majors = sbert.encode(majors, convert_to_numpy=True, normalize_embeddings=True)
    sim_major_all = cosine_similarity(emb_doc, emb_majors)[0]

    texts_res_all = [' | '.join(p['researchs']) for p in profiles]
    emb_res_all = sbert.encode(texts_res_all, convert_to_numpy=True, normalize_embeddings=True)
    sim_res_all = cosine_similarity(emb_doc, emb_res_all)[0]

    texts_proj_all = [' | '.join(p['projects']) for p in profiles]
    emb_proj_all = sbert.encode(texts_proj_all, convert_to_numpy=True, normalize_embeddings=True)
    sim_proj_all = cosine_similarity(emb_doc, emb_proj_all)[0]

    texts_fp_all = [' | '.join(p['fingerprints']) for p in profiles]
    emb_fp_all = sbert.encode(texts_fp_all, convert_to_numpy=True, normalize_embeddings=True)
    sim_fp_all = cosine_similarity(emb_doc, emb_fp_all)[0]

    stage1_scores = (
        STAGE1_WEIGHTS[0] * sim_major_all +
        STAGE1_WEIGHTS[1] * sim_res_all  +
        STAGE1_WEIGHTS[2] * sim_proj_all +
        STAGE1_WEIGHTS[3] * sim_fp_all
    )

    k150 = min(150, len(profiles))
    idxs1 = np.argsort(stage1_scores)[::-1][:k150]
    cand1 = [profiles[i] for i in idxs1]

    # cand1ì— í•´ë‹¹í•˜ëŠ” ìœ ì‚¬ë„ subset
    sim_major = sim_major_all[idxs1]
    sim_proj  = sim_proj_all[idxs1]
    sim_fp    = sim_fp_all[idxs1]

    # ----- Stage2 (ì—°ë„ ê°€ì¤‘ researchs) -----
    current_year = datetime.datetime.now().year
    years = list(range(current_year, current_year - 25, -1))

    raw_year_weights = np.array(
        [YEAR_WEIGHTS.get(y, DEFAULT_YEAR_WEIGHT) for y in years],
        dtype=float
    )
    year_weights = raw_year_weights / raw_year_weights.sum()

    all_year_sims = []
    for p in cand1:
        sims_per_year = []
        for y in years:
            entries = [e for e in p['researchs'] if str(y) in e]
            text = ' | '.join(entries) if entries else ''
            if text:
                sim_val = cosine_similarity(
                    emb_doc,
                    sbert.encode([text], convert_to_numpy=True, normalize_embeddings=True)
                )[0][0]
            else:
                sim_val = 0.0
            sims_per_year.append(sim_val)
        all_year_sims.append(sims_per_year)
    all_year_sims = np.array(all_year_sims)

    research_year_scores = np.dot(all_year_sims, year_weights)

    # Stage2 ìµœì¢… ìŠ¤ì½”ì–´
    stage2_scores = (
        STAGE2_WEIGHTS[0] * sim_major +
        STAGE2_WEIGHTS[1] * research_year_scores +
        STAGE2_WEIGHTS[2] * sim_proj +
        STAGE2_WEIGHTS[3] * sim_fp
    )

    # ----- ì—˜ë³´ìš° ì»· -----
    thr = find_elbow_threshold(stage2_scores)
    labels = stage2_scores >= thr

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
    order = np.argsort(stage2_scores)[::-1]

    # ì „ì²´ í›„ë³´(ì»· ì „) + ì¶”ì²œ í›„ë³´(ì»· í›„)
    rows_all = []
    rows_rec = []
    for rank, idx in enumerate(order, start=1):
        p = cand1[idx]
        row = {
            "rank":  rank,
            "name":  p.get("name", ""),
            "major": p.get("major", ""),
            "email": p.get("email", ""),
            "score": float(stage2_scores[idx]),
            "label": bool(labels[idx]),
        }
        rows_all.append(row)
        if labels[idx]:
            rows_rec.append(row)

    # í™”ë©´ìš©: ìƒìœ„ Nëª… (score + label ë‘˜ ë‹¤ ë³´ì—¬ì¤Œ)
    preview_all = rows_all[:top_k_preview]

    # CSVëŠ” ì—¬ì „íˆ label=Trueë§Œ
    fieldnames = ["rank", "name", "major", "email", "score", "label"]
    csv_rec = csv_bytes_from_rows(rows_rec, fieldnames)

    meta = {
        "threshold": float(thr),
        "n_candidates": len(cand1),
        "n_recommended": len(rows_rec),
    }

    return preview_all, csv_rec, meta, rows_rec



# ===== RAG ìš”ì•½ + ë§¤ì¹­: ì—…ë¡œë“œ ì¦‰ì‹œ ìë™ ì‹¤í–‰ =====
uploaded_file = st.file_uploader("ìš”ì•½Â·ë§¤ì¹­í•  PDF ê³µê³ ë¬¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

if uploaded_file:
    # ìƒë‹¨ ë©”íƒ€ í‘œì‹œ
    found, department, notice_link, folder_title = find_file_and_get_info(BASE_DATA_PATH, uploaded_file.name)
    if found:
        st.info(
            f"**ğŸ“‚ ê³µê³  ì œëª©:** {folder_title}  \n"
            f"**ğŸ¢ ì£¼ê´€ ê¸°ê´€:** {department}  \n"
            f"**ğŸ”— ê³µê³  ë§í¬:** {notice_link}"
        )

    # â‘  PDF â†’ í…ìŠ¤íŠ¸
    with st.spinner("â‘  PDF ë¶„ì„ ì¤‘ (í…ìŠ¤íŠ¸ ì¶”ì¶œ)â€¦"):
        pdf_bytes = uploaded_file.getvalue()
        documents = extract_text_from_pdf(pdf_bytes)
        if not documents:
            st.stop()

    # â‘¡ RAG ì¤€ë¹„
    with st.spinner("â‘¡ RAG ì¤€ë¹„ ì¤‘ (ì²­í¬/ì„ë² ë”©/ì¸ë±ì‹±)â€¦"):
        splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)
        texts = splitter.split_documents(documents)
        if not texts:
            st.error("í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë‹¨ìœ„ë¡œ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # Google ì„ë² ë”© ìš°ì„ , ì‹¤íŒ¨ ì‹œ HF í´ë°±
        try:
            g_emb = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            vector_store = FAISS.from_documents(texts, g_emb)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                st.warning("Google ì„ë² ë”© ì¿¼í„° ì´ˆê³¼ë¡œ ë¡œì»¬ ì„ë² ë”©ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                hf_emb = HFEmbeddings(
                    model_name=HF_EMBED_MODEL_FOR_RAG,
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"normalize_embeddings": True},
                )
                vector_store = FAISS.from_documents(texts, hf_emb)
            else:
                raise

        retriever = vector_store.as_retriever(search_kwargs={"k": 7})

    # â‘¢ Gemini ìš”ì•½
    with st.spinner("â‘¢ ìš”ì•½ ìƒì„± ì¤‘ (Gemini)â€¦"):
        llm = ChatGoogleGenerativeAI(model="models/gemini-2.0-flash", temperature=0)
        stuff_prompt = ChatPromptTemplate.from_template(get_prompt_template())
        chain = (
            {"context": retriever | RunnableLambda(_format_docs), "input": RunnablePassthrough()}
            | stuff_prompt
            | llm
            | StrOutputParser()
        )
        question = "ì´ ê³µê³ ë¬¸ì˜ ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ì˜ 'ì¶œë ¥ ì–‘ì‹'ì— ë§ì¶°ì„œ ì•„ì£¼ ìƒì„¸í•˜ê²Œ ìš”ì•½í•´ì¤˜."
        result_text = chain.invoke(question)
        summary = clean_final_output(result_text)

    st.success("âœ… ìš”ì•½ ì™„ë£Œ")
    st.text_area("ğŸ“Œ ìµœì¢… ìš”ì•½ ê²°ê³¼", summary, height=450)

    # â‘£ SBERT ë§¤ì¹­
    if not os.path.exists(profiles_path):
        st.error(f"í”„ë¡œí•„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {profiles_path}")
        st.stop()

    with st.spinner("â‘£ ìš”ì•½ ê¸°ë°˜ êµìˆ˜ ë§¤ì¹­ ê³„ì‚° ì¤‘ (SBERT)â€¦"):
        try:
            preview_rec, csv_rec, meta, recommended_list = run_matching(
                summary_text=summary,
                profiles_file=profiles_path,
                top_k_preview=TOP_K_PREVIEW
            )
        except Exception as e:
            st.exception(e)
            st.stop()

    st.success(
        f"âœ… ë§¤ì¹­ ì™„ë£Œ | ì„ê³„ê°’={meta['threshold']:.4f} | "
        f"í›„ë³´ {meta['n_candidates']}ëª… ì¤‘ ì¶”ì²œ {meta['n_recommended']}ëª…"
    )

    # ìµœì¢… ì¶”ì²œ í…Œì´ë¸” í‘œì‹œ (score ì—†ì´)
    st.subheader("ì¶”ì²œ ëŒ€ìƒ (label=True)")
    st.dataframe(preview_rec, use_container_width=True)

    # ì „ì²´ ì¶”ì²œ rawë„ ë³´ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ
    # st.json(recommended_list)

    # ì¶”ì²œ CSV ë‹¤ìš´ë¡œë“œ
    st.download_button(
        "ì¶”ì²œ í›„ë³´ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_rec,
        file_name=f"match_results_recommended_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        use_container_width=True,
    )

else:
    st.info("ğŸ‘† PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ìš”ì•½ â†’ ë§¤ì¹­ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")